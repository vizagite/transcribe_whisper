{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/vizagite/transcribe_whisper/blob/main/transcribe.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "\n",
        "We are using Distilled whisper large model v3 with fp16 quantization which is pretty fast for inference and best results (just around 2 min for 2 hour audio transcription on free T4 Colab GPU with 90%+ accuracy)"
      ],
      "metadata": {
        "id": "lTszCKGXzd4z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhkP6V9Js8jc"
      },
      "outputs": [],
      "source": [
        "!pip install -q pipx && apt install python3.11-venv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pipx install insanely-fast-whisper --force --pip-args=\"--ignore-requires-python\" #lets use package instead of rewriting model pipeline"
      ],
      "metadata": {
        "id": "BSDtaeb1tC8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install flash attention if you are on Pro colab plan, it wont work on free and below Ampere GPUs\n",
        "# !pipx runpip insanely-fast-whisper install flash-attn --no-build-isolation --ignore-requires-python"
      ],
      "metadata": {
        "id": "XETH_YlntNfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# since we are running headless, detaching inline things\n",
        "os.environ[\"MPLBACKEND\"] = \"Agg\"\n",
        "os.environ[\"PATH\"] += \":/root/.local/bin\"\n",
        "import matplotlib"
      ],
      "metadata": {
        "id": "4_azeU1rtPNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test if setup is good\n",
        "!insanely-fast-whisper --model-name distil-whisper/distil-large-v3 --file-name https://huggingface.co/datasets/reach-vb/random-audios/resolve/main/ted_60.wav"
      ],
      "metadata": {
        "id": "A4MOw16LtXpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you can pass --hf-token ... (your huggingace token - https://hf.co/settings/tokens and fill details at https://huggingface.co/pyannote/speaker-diarization-3.1 form, if you want diarized transcript)\n",
        "# upload audio file to colab and use the same filename below\n",
        "\n",
        "!insanely-fast-whisper --model-name distil-whisper/distil-large-v3 --file-name audio_file.mp3 --task transcribe --transcript-path transcript.json --min-speakers 2"
      ],
      "metadata": {
        "id": "ng-EnPcVtoaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets print the above transcript json with proper format\n",
        "\n",
        "import json\n",
        "import datetime\n",
        "\n",
        "def format_timestamp(seconds):\n",
        "  seconds = int(seconds)\n",
        "  return str(datetime.timedelta(seconds=seconds))\n",
        "\n",
        "def process_json(file_path=\"transcript.json\"):\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found.\")\n",
        "        return\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: Invalid JSON format in '{file_path}'.\")\n",
        "        return\n",
        "\n",
        "    if \"chunks\" in data:\n",
        "      for chunk in data[\"chunks\"]:\n",
        "          if \"timestamp\" in chunk and \"text\" in chunk:\n",
        "              start_time = chunk[\"timestamp\"][0]\n",
        "              formatted_time = format_timestamp(start_time)\n",
        "              print(f\"{formatted_time} - {chunk['text']}\")\n",
        "      return\n",
        "    # if diarized above with another model\n",
        "    # person_dict = {\"SPEAKER_01\": \"some name\", \"SPEAKER_02\": \"..\", \"SPEAKER_03\": \"..\"}\n",
        "    for chunk in data[\"speakers\"]:\n",
        "        if \"timestamp\" in chunk and \"text\" in chunk:\n",
        "            start_time = chunk[\"timestamp\"][0]\n",
        "            formatted_time = format_timestamp(start_time)\n",
        "            print(f\"{formatted_time} - {person_dict[chunk['speaker']]} - {chunk['text']}\")\n",
        "\n",
        "process_json()"
      ],
      "metadata": {
        "id": "TDeBW9jTt49_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}